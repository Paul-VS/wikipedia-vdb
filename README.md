# Wikipedia Processing Scripts

This repository contains two scripts for processing Wikipedia articles: `extract-wiki.py` and `create-wiki-vdb.py`. These scripts can be used to convert a Wikipedia dump into Parquet files and store embeddings of the articles in a PostgreSQL database.

## Extract Wiki Script

The `extract-wiki.py` script processes a Wikipedia dump file and converts it into Parquet files. It performs the following steps:

1. Reads the Wikipedia dump file in XML format.
2. Extracts the raw byte data for each Wikipedia article from the compressed `.bz2` file.
3. Parses the raw byte data to extract the article ID, title, and text.
4. Cleans the Wikipedia text by removing wiki markup and consecutive empty lines.
5. Splits the cleaned text into smaller chunks.
6. Writes the processed data to Parquet files using the `snappy` compression method.

To use the script, make sure to set the appropriate file paths, such as the path to the Wikipedia dump file and the output directory for the Parquet files. Adjust the processing parameters as needed, such as the number of processors and the number of parallel blocks.

## Store Embeddings Script

The `create-wiki-vdb.py` script processes the Parquet files generated by the `extract-wiki.py` script and stores the embeddings of the articles in a PostgreSQL database. It performs the following steps:

1. Reads the Parquet file into a pandas DataFrame.
2. Cleans the article text by removing unnecessary information.
3. Splits the cleaned text into smaller chunks.
4. Computes embeddings for each chunk using the SentenceTransformer model.
5. Establishes a connection to a PostgreSQL database.
6. Creates a table to store the article embeddings.
7. Inserts the article title, chunk, and embedding into the database table.

To use the script, make sure to set the appropriate database connection parameters and adjust the batch size and SentenceTransformer model as needed.

## Requirements

Both scripts require the following dependencies:

- `pandas`
- `pyarrow`
- `lxml`
- `mwparserfromhell`
- `tqdm`
- `multiprocessing`
- `psycopg2`
- `sentence-transformers`

Please install these dependencies before running the scripts.

## Running the Scripts

To run the scripts, follow these steps:

1. Clone the repository and navigate to the project directory.
2. Install the required dependencies.
3. Update the script-specific configuration parameters, such as file paths and database connection details.
4. Run the scripts using the command `python extract-wiki.py` and `python create-wiki-vdb.py`.
5. Monitor the progress and check the output files or database for the results.

Please note that the scripts assume you have the necessary Wikipedia dump file and a running PostgreSQL database with the correct configuration.

## License

This project is licensed under the [MIT License](LICENSE).

Feel free to modify and adapt the scripts to suit your needs!
